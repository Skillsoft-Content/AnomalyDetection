{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f75f7c5",
   "metadata": {},
   "source": [
    "## ANOMALY DETECTION PART 1 EXERCISE ANSWERS ##\n",
    "#### Exercise 1 ####\n",
    "#### Task 1\n",
    "##### Import the required packages\n",
    "##### Set the working directory to data directory\n",
    "##### Print the working directory and the plot directory\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "np.set_printoptions(suppress=True) #<- suppress scientific notations\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# Set 'main_dir' to location of the project folder\n",
    "from pathlib import Path \n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abecc120",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Read in `seismic-hazards.csv` to a new dataframe `hazard`.\n",
    "##### Read the documentation to understand the variables https://archive.ics.uci.edu/ml/datasets/seismic-bumps#\n",
    "##### Explore the dataset by printing its head, info and shape.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard = pd.read_csv(str(data_dir) + '/seismic-hazard.csv')\n",
    "hazard.head()\n",
    "hazard.shape\n",
    "hazard.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd970b9",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### The target variable in this dataset is `class`.\n",
    "##### Print the value counts of the target and also check the percentage of the outlier class.\n",
    "##### Plot the distribution of the target.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c718b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard['class'].value_counts()\n",
    "print(hazard['class'].value_counts()/ len(hazard))\n",
    "hazard['class'].value_counts().plot(kind = 'bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165bcc17",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Drop all the non-numerical columns and target variable `class`. Save this subset as `hazard_dbscan` and print its head.\n",
    "##### Check how many NAs are in each column and impute them with mean, if required.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f961e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_dbscan = hazard.drop(['seismic','seismoacoustic','shift','ghazard','class','id'], axis = 1)\n",
    "hazard_dbscan.head()\n",
    "hazard_dbscan.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eba815",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Scale `hazard_dbscan` using the `StandardScaler` function.\n",
    "##### Name the scaled dataframe as `hazard_dbscan_scaled`. \n",
    "##### Convert `hazard_dbscan_scaled` back to a Pandas dataframe and make sure that the column names are the same as before.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e735263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "hazard_dbscan_scaled = scaler.fit_transform(hazard_dbscan)\n",
    "hazard_dbscan_scaled = pd.DataFrame(hazard_dbscan_scaled, columns=hazard_dbscan.columns)\n",
    "hazard_dbscan_scaled.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59821c7f",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Run the DBSCAN model on `ex_db`.\n",
    "##### For now, we set Ïµ to have a radius of 0.4.\n",
    "##### We set MinPts (min_samples in the function) to 5.\n",
    "##### What do those two parameters mean?\n",
    "##### Check how many clusters we have.\n",
    "##### How many outliers do you have?\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f892aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=0.4, min_samples = 5)\n",
    "clusters = dbscan.fit_predict(hazard_dbscan_scaled)\n",
    "It means that we want 5 samples in a neighborhood with a radius of 0.4.\n",
    "unique, counts = np.unique(clusters, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "We have 12 clusters and 838 outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d379b",
   "metadata": {},
   "source": [
    "#### Exercise 2 ####\n",
    "#### Task 1\n",
    "##### Idenify the optimal eps using NearestNeighbors().\n",
    "##### Set number of neighbors to be `10` and fit the model to `hazard_dbscan_scaled`\n",
    "##### Calculate the distances of every point with it's 10 closest neighbors\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb7228",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = NearestNeighbors(n_neighbors=10)\n",
    "nbrs = nn_model.fit(hazard_dbscan_scaled)\n",
    "distances, indices = nbrs.kneighbors(hazard_dbscan_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a876466",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Calculate average distance for each point with its 10 neighbors and sort the distances\n",
    "##### Plot the sorted distances\n",
    "##### Zoom the plot by filtering the distances after index 2400 \n",
    "##### Find the optimal `eps` distance from the elbow plot\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1dde9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.mean(distances,axis=1)\n",
    "distances = np.sort(distances, axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "elbow = ax.plot(distances)\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('Distance (eps)')\n",
    "plt.show()\n",
    "fig, ax = plt.subplots()\n",
    "elbow_zoom = ax.plot(distances[2400:])\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('Distance (eps)')\n",
    "plt.show()\n",
    "Optimal eps distance is 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c1f3b",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Run a new DBSCAN analysis with the optimized parameters.\n",
    "##### Check the number of clusters.\n",
    "##### Assign the cluster results to `hazard_dbscan`. Replace the anomaly cluster points with `1` and rest cluster points to `0`\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff792a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=6, min_samples = 10)\n",
    "optimized_clusters = dbscan.fit_predict(hazard_dbscan_scaled)\n",
    "# Check the number of clusters\n",
    "unique, counts = np.unique(optimized_clusters, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "hazard_dbscan['cluster'] = optimized_clusters\n",
    "hazard_dbscan.loc[hazard_dbscan['cluster'] >= 0,'cluster'] = 0\n",
    "hazard_dbscan.loc[hazard_dbscan['cluster'] == -1,'cluster'] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0d8900",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Visualize the anomalies detected using a scatterplot\n",
    "##### Plot a scatterplot with `energy` and `maxenergy`\n",
    "##### Plot a scatterplot with `genergy` and `maxenergy`\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c69cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(hazard_dbscan['energy'], hazard_dbscan['maxenergy'], c = hazard_dbscan['cluster'], alpha = 0.8)\n",
    "ax.legend(*sc.legend_elements())\n",
    "plt.xlabel('Amount of transaction')\n",
    "plt.ylabel('Origin new balance after transaction')\n",
    "plt.show()\n",
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(hazard_dbscan['genergy'], hazard_dbscan['energy'], c = hazard_dbscan['cluster'], alpha = 0.8)\n",
    "ax.legend(*sc.legend_elements())\n",
    "plt.xlabel('Amount of transaction')\n",
    "plt.ylabel('Origin new balance after transaction')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d504b5",
   "metadata": {},
   "source": [
    "## ANOMALY DETECTION PART 2 EXERCISE ANSWERS ##\n",
    "#### Exercise 3 ####\n",
    "#### Task 1\n",
    "##### Import the required packages\n",
    "##### Set the working directory to data directory\n",
    "##### Print the working directory and the plot directory\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d6ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "np.set_printoptions(suppress=True) #<- suppress scientific notations\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# Set 'main_dir' to location of the project folder\n",
    "from pathlib import Path \n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e29af0",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Load the `PJM_Load_hourly.csv` dataset and print the head. Save as `pjm_load`.\n",
    "##### Convert the `Datatime` variable from type `object` to `datetime`. Check its datatype after type-conversion\n",
    "##### Filter the data to include values post-year 2000\n",
    "##### Visualize the time series data using a line plot\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a979599",
   "metadata": {},
   "outputs": [],
   "source": [
    "pjm_load = pd.read_csv(str(data_dir) + \"/PJM_Load_hourly.csv\")\n",
    "pjm_load.head()\n",
    "pjm_load['Datetime'] = pd.to_datetime(pjm_load['Datetime'])\n",
    "pjm_load.info()\n",
    "pjm_load = pjm_load[pjm_load['Datetime'] > '2000-01-01 00:00:00']\n",
    "pjm_load.plot(x='Datetime', y='PJM_Load_MW', figsize=(17,6))\n",
    "plt.xlabel('Date time')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Energy consumption (MW) at each hour')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f12a2",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Scale the variable `PJM_Load_MW` and save the dataset as `pjm_load `. \n",
    "##### Find the optimal eps value using the k-distance approach\n",
    "##### Set the number of neighbors in NearestNeighbors() to be 5.\n",
    "##### Zoom the plot for values after `17000`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307bdb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# Scale the dataframe.\n",
    "ex_pjm_energy_scaled = scaler.fit_transform(pd.DataFrame(pjm_load['PJM_Load_MW']))\n",
    "pd.DataFrame(ex_pjm_energy_scaled).describe()\n",
    "nn_model = NearestNeighbors(n_neighbors=5)\n",
    "nbrs = nn_model.fit(ex_pjm_energy_scaled)\n",
    "distances, indices = nbrs.kneighbors(ex_pjm_energy_scaled)\n",
    "distances = np.mean(distances,axis=1)\n",
    "distances = np.sort(distances, axis=0)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(distances)\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('Distance (eps)')\n",
    "plt.show()\n",
    "plt.plot(distances[17000:])\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('Distance (eps)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5bbc58",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Apply the DBSCAN algorithm on `ex_pjm_energy_scaled` with `optimal eps` and `min_samples:5`.\n",
    "##### Visualize the anomalies.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfa64b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_dbscan_energy = DBSCAN(eps = 0.01, metric='euclidean',min_samples=5, n_jobs = -1)\n",
    "# pjm_time_series['anomaly'] = dbscan_energy.fit_predict(pd.DataFrame(pjm_time_series['PJME_MW']))\n",
    "pjm_load['anomaly'] = ex_dbscan_energy.fit_predict(pd.DataFrame(ex_pjm_energy_scaled))\n",
    "# visualization\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "a = pjm_load.loc[pjm_load['anomaly'] == -1, ['Datetime', 'PJM_Load_MW']] #anomaly\n",
    "ax.plot(pjm_load['Datetime'], pjm_load['PJM_Load_MW'], color='blue', label = 'Normal')\n",
    "ax.scatter(a['Datetime'],a['PJM_Load_MW'], color='red', label = 'Anomaly')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad4424",
   "metadata": {},
   "source": [
    "#### Exercise 4 ####\n",
    "#### Task 1\n",
    "##### Read in `seismic-hazards.csv` to a new dataframe `hazard`.\n",
    "##### Use the `hazard` dataset to prepare for decision tree modeling\n",
    "##### Drop the `id` column, convert the categorical variables to dummies, and append them to the original dataframe.\n",
    "##### The variables are: seismic, seismoacoustic, shift, and ghazard. Note that the values of these variables are already coded.\n",
    "##### Hint: Columns can be sent listed together in pd.get_dummies()\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f765b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard = pd.read_csv(str(data_dir) + '/seismic-hazard.csv')\n",
    "hazard = hazard.drop(['id'], axis = 1)\n",
    "colname = pd.get_dummies(hazard[['seismic','seismoacoustic','shift','ghazard']], drop_first = True)\n",
    "hazard = pd.concat([hazard, colname], axis = 1)\n",
    "hazard.drop(['seismic','seismoacoustic','shift','ghazard'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39101238",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Separate the target as `y_ex` and other columns as `X_ex`.\n",
    "##### Separate training and test data as `X_train_ex`, `X_test_ex`, `y_train_ex`, `y_test_ex` with 70:30 partition and fit a DecisionTreeClassifier with `max_depth: 10`\n",
    "##### Set random seed to 1\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc4c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate predictors and target.\n",
    "y_ex = hazard['class']\n",
    "X_ex = hazard.drop(['class'], axis = 1)\n",
    "np.random.seed(1)\n",
    "X_train_ex, X_test_ex, y_train_ex, y_test_ex = train_test_split(X_ex, y_ex, test_size = 0.30)\n",
    "dtree = DecisionTreeClassifier(max_depth = 10)\n",
    "dtree.fit(X_train_ex, y_train_ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e554e",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Predict on the training and test datasets.\n",
    "##### Print the testing accuracy.\n",
    "##### Print the confusion matrix for training and test data.\n",
    "##### Find the percentage of accurate hazards on training and test data.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882901dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_y_train_pred = dtree.predict(X_train_ex)\n",
    "dtree_y_test_pred = dtree.predict(X_test_ex)\n",
    "dtree_accuracy_ex = metrics.accuracy_score(y_test_ex, dtree_y_test_pred)\n",
    "print(\"Accuracy of test data:\\t\", dtree_accuracy_ex)\n",
    "print('Confusion Matrix - Training Dataset')\n",
    "print(pd.crosstab(y_train_ex, dtree_y_train_pred, rownames = ['True'], colnames = ['Predicted'], margins = True))\n",
    "print('Percentage of accurate hazards found', 92/116)\n",
    "print('Confusion Matrix - Testing Dataset')\n",
    "print(pd.crosstab(y_test_ex, dtree_y_test_pred, rownames = ['True'], colnames = ['Predicted'], margins = True))\n",
    "print('Percentage of accurate hazards found', 8/54)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb4066",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Find TPR and TNR.\n",
    "##### Save the metric to a new performance dataframe with the name `performance_df_ex`.\n",
    "##### Interpret the results.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test_ex, dtree_y_test_pred).ravel()\n",
    "# Find TNR.\n",
    "non_fraud_eval = tn / (tn + fp)\n",
    "print(non_fraud_eval)\n",
    "# Find TPR.\n",
    "fraud_eval = tp / (tp + fn)\n",
    "print(fraud_eval)\n",
    "performance_df_ex = pd.DataFrame(columns = ['model_name', 'TPR', 'TNR'])\n",
    "s = pd.Series(['DecisionTree_baseline', fraud_eval, non_fraud_eval], \n",
    "              index=['model_name', 'TPR', 'TNR'])\n",
    "performance_df_ex = performance_df_ex.append(s, ignore_index = True)\n",
    "performance_df_ex\n",
    "#TNR is very high but TPR is close to 10% which is really odd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5074e93e",
   "metadata": {},
   "source": [
    "#### Exercise 5 ####\n",
    "#### Task 1\n",
    "##### Set random_state as 1 and create SMOTE samples to training dataset as `X_train_new_ex` and `y_train_new_ex` to balance the classes.\n",
    "##### Print the shape of the training data before resampling and after resampling.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d0ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state = 2)\n",
    "X_train_new_ex, y_train_new_ex = sm.fit_resample(X_train_ex, y_train_ex)\n",
    "# Shape of X_train_ex.\n",
    "print(X_train_ex.shape)\n",
    "# Shape of X_train_new_ex.\n",
    "print(X_train_new_ex.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b74985",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Plot the target variable of the resampled dataset (`y_train_new_ex`)\n",
    "##### How are the data points distributed?\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c6bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe that data has been balanced.\n",
    "pd.Series(y_train_new_ex).value_counts().plot.bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a483df9",
   "metadata": {},
   "source": [
    "- The classes are balanced now after SMOTE resampling\n",
    "#### Task 3\n",
    "##### Fit the DecisionTree Classifer to the resampled dataset and predict on train and test data.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79bb133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model.\n",
    "dtree.fit(X_train_new_ex, y_train_new_ex)\n",
    "# Prediction for training data.\n",
    "train_pred_sm = dtree.predict(X_train_new_ex)\n",
    "# Prediction for test data.\n",
    "test_pred_sm = dtree.predict(X_test_ex)\n",
    "train_pred_sm = dtree.predict(X_train_new_ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cc5ffd",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Print the confusion matrix for training and test data.\n",
    "##### Analyze the result.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa011960",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.crosstab(y_train_new_ex, train_pred_sm, rownames = ['True'], colnames = ['Predicted'], margins = True))\n",
    "print(pd.crosstab(y_test_ex, test_pred_sm, rownames = ['True'], colnames = ['Predicted'], margins = True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed15aa23",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Find the TPR and TNR and save the result.\n",
    "##### Compare the result to our baseline DecisionTree model and interpret.\n",
    "##### Optional: If you wish to run other classification algorithms and compare results then save the dataframe as a pickle. Make sure to give it a name differing from slides pickle.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find TPR and TNR and save the result.\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_ex, test_pred_sm).ravel()\n",
    "non_hazard_eval = tn / (tn + fp)\n",
    "print(non_hazard_eval)\n",
    "hazard_eval = tp / (tp + fn)\n",
    "print(hazard_eval)\n",
    "s = pd.Series(['SMOTE', hazard_eval, non_hazard_eval], \n",
    "              index = ['model_name', 'TPR', 'TNR'])\n",
    "performance_df_ex = performance_df_ex.append(s, ignore_index = True)\n",
    "performance_df_ex\n",
    "#Performing SMOTE has shown improvement in predicting hazard as we see an increase in TPR\n",
    "pickle.dump(performance_df_ex, open(str(data_dir) + \"/ex_performance_anomalies.sav\",\"wb\" ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
